import os
import json
from typing import AsyncGenerator
from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.staticfiles import StaticFiles
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from dotenv import load_dotenv
from exa_py import Exa
from cerebras.cloud.sdk import Cerebras
from pathlib import Path

# Load environment variables from .env file
load_dotenv()

# Get API keys from environment variables
EXA_API_KEY = os.getenv("EXA_API_KEY")
CEREBRAS_API_KEY = os.getenv("CEREBRAS_API_KEY")

# Check if API keys are set
if not EXA_API_KEY or not CEREBRAS_API_KEY:
    raise RuntimeError("API keys for Exa and Cerebras must be set in a .env file.")

# Initialize clients
exa = Exa(api_key=EXA_API_KEY)
client = Cerebras(api_key=CEREBRAS_API_KEY)

app = FastAPI(
    title="Mathmatika Research API",
    description="AI-powered research API with multi-agent capabilities and Manim visualization",
    version="1.0.0",
    openapi_tags=[
        {
            "name": "chat",
            "description": "Streaming chat endpoints for conversational AI"
        },
        {
            "name": "research",
            "description": "Research endpoints with varying depth and strategies"
        },
        {
            "name": "manim",
            "description": "Manim animation generation for mathematical concepts"
        }
    ]
)

# Configure CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["http://localhost:3000"],  # Next.js dev server
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Import and include Manim router
try:
    # Try absolute import first (when running as module)
    from src.manim import router as manim_router
except ImportError:
    try:
        # Try relative import (when running from src directory)
        from manim import router as manim_router
    except ImportError:
        # If both fail, manim module might not be available
        manim_router = None
        print("Warning: Manim module not found. Manim endpoints will not be available.")

if manim_router:
    app.include_router(manim_router)

# Mount static files for video serving (if directory exists)
static_dir = Path(__file__).parent.parent / "static"
if static_dir.exists():
    app.mount("/static", StaticFiles(directory=str(static_dir)), name="static")

# Request/Response Models
class ResearchRequest(BaseModel):
    query: str = Field(..., description="Research query or question", min_length=3)

class ResearchResponse(BaseModel):
    query: str
    sources: int
    response: str

class MultiAgentResearchResponse(BaseModel):
    query: str
    subagents: int
    total_sources: int
    synthesis: str

# Chat Models
class ChatMessage(BaseModel):
    role: str = Field(..., description="Message role: 'user', 'assistant', or 'system'")
    content: str = Field(..., description="Message content")

class ChatRequest(BaseModel):
    message: str = Field(..., description="User message", min_length=1)
    conversation_history: list[ChatMessage] = Field(
        default_factory=list,
        description="Previous conversation messages for context"
    )

# Web Search Function
def search_web(query: str, num_results: int = 5):
    """Search the web using Exa's auto search"""
    try:
        result = exa.search_and_contents(
            query,
            type="auto",
            num_results=num_results,
            text={"max_characters": 1000}
        )
        return result.results
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Exa API error: {e}")

# AI Analysis Function
def ask_ai(prompt: str):
    """Get AI response from Cerebras"""
    try:
        chat_completion = client.chat.completions.create(
            messages=[
                {
                    "role": "user",
                    "content": prompt,
                }
            ],
            model="llama-4-scout-17b-16e-instruct",
            max_tokens=600,
            temperature=0.2
        )
        return chat_completion.choices[0].message.content
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Cerebras API error: {e}")

# Research Function
@app.post("/research", response_model=ResearchResponse, tags=["research"])
def research_topic(request: ResearchRequest):
    """
    Basic research with AI synthesis

    - Searches 5 web sources
    - Returns 2-3 sentence summary
    - Provides 3 key insights
    """
    query = request.query
    print(f"ðŸ” Researching: {query}")

    # Search for sources
    results = search_web(query, 5)
    print(f"ðŸ“Š Found {len(results)} sources")

    # Get content from sources
    sources = []
    for result in results:
        content = result.text
        title = result.title
        if content and len(content) > 200:
            sources.append({
                "title": title,
                "content": content
            })

    print(f"ðŸ“„ Scraped {len(sources)} sources")

    if not sources:
        return {"summary": "No sources found", "insights": []}

    # Create context for AI analysis
    context = f"Research query: {query}\n\nSources:\n"
    for i, source in enumerate(sources[:4], 1):
        context += f"{i}. {source['title']}: {source['content'][:400]}...\n\n"
        

    # Ask AI to analyze and synthesize
    prompt = f"""{context}

Based on these sources, provide:
1. A comprehensive summary (2-3 sentences)
2. Three key insights as bullet points

Format your response exactly like this:
SUMMARY: [your summary here]

INSIGHTS:
- [insight 1]
- [insight 2]
- [insight 3]"""

    response = ask_ai(prompt)
    print("ðŸ§  Analysis complete")

    return {"query": query, "sources": len(sources), "response": response}

# Deeper Research Function
@app.post("/deep-research", response_model=ResearchResponse, tags=["research"])
def deeper_research_topic(request: ResearchRequest):
    """
    Two-layer research for better depth

    - Layer 1: Initial search (6 sources)
    - AI identifies follow-up question
    - Layer 2: Follow-up search (4 sources)
    - Final synthesis with depth analysis
    """
    query = request.query
    print(f"ðŸ” Researching: {query}")

    # Layer 1: Initial search
    results = search_web(query, 6)
    sources = []
    for result in results:
        if result.text and len(result.text) > 200:
            sources.append({"title": result.title, "content": result.text})

    print(f"Layer 1: Found {len(sources)} sources")

    if not sources:
        return {"summary": "No sources found", "insights": []}

    # Get initial analysis and identify follow-up topic
    context1 = f"Research query: {query}\n\nSources:\n"
    for i, source in enumerate(sources[:4], 1):
        context1 += f"{i}. {source['title']}: {source['content'][:300]}...\n\n"

    follow_up_prompt = f"""{context1}

Based on these sources, what's the most important follow-up question that would deepen our understanding of \"{query}\"?

Respond with just a specific search query (no explanation):"""

    follow_up_query = ask_ai(follow_up_prompt).strip().strip('"')

    # Layer 2: Follow-up search
    print(f"Layer 2: Investigating '{follow_up_query}'")
    follow_results = search_web(follow_up_query, 4)

    for result in follow_results:
        if result.text and len(result.text) > 200:
            sources.append({"title": f"[Follow-up] {result.title}", "content": result.text})

    print(f"Total sources: {len(sources)}")

    # Final synthesis
    all_context = f"Research query: {query}\nFollow-up: {follow_up_query}\n\nAll Sources:\n"
    for i, source in enumerate(sources[:7], 1):
        all_context += f"{i}. {source['title']}: {source['content'][:300]}...\n\n"

    final_prompt = f"""{all_context}

Provide a comprehensive analysis:

SUMMARY: [3-4 sentences covering key findings from both research layers]

INSIGHTS:
- [insight 1]
- [insight 2]
- [insight 3]
- [insight 4]

DEPTH GAINED: [1 sentence on how the follow-up search enhanced understanding]"""

    response = ask_ai(final_prompt)
    return {"query": query, "sources": len(sources), "response": response}

# Multi-Agent Research Function
@app.post("/multi-agent-research", response_model=MultiAgentResearchResponse, tags=["research"])
def anthropic_multiagent_research(request: ResearchRequest):
    """
    Multi-agent research with parallel execution

    - Lead agent decomposes query into 3 subtasks
    - 3 specialized subagents work in parallel
    - Each subagent searches 2 sources
    - Lead agent synthesizes all findings
    - Returns executive summary and integrated insights
    """
    query = request.query
    print(f"ðŸ¤– Anthropic Multi-Agent Research: {query}")
    print("-" * 50)

    # Step 1: Lead Agent - Task Decomposition & Delegation
    print("ðŸ‘¨â€ðŸ’¼ LEAD AGENT: Planning and delegating...")

    delegation_prompt = f"""You are a Lead Research Agent. Break down this complex query into 3 specialized subtasks for parallel execution: \"{query}\"\n
For each subtask, provide:
- Clear objective
- Specific search focus
- Expected output

SUBTASK 1: [Core/foundational aspects]
SUBTASK 2: [Recent developments/trends]
SUBTASK 3: [Applications/implications]\n
Make each subtask distinct to avoid overlap."""

    plan = ask_ai(delegation_prompt)
    print("  âœ“ Subtasks defined and delegated")

    # Step 2: Simulate Parallel Subagents (simplified for demo)
    print("\nðŸ” SUBAGENTS: Working in parallel...")

    # Extract subtasks and create targeted searches
    subtask_searches = [
        f"{query} fundamentals principles",  # Core aspects
        f"{query} latest developments",  # Recent trends
        f"{query} applications real world"    # Implementation
    ]

    subagent_results = []
    for i, search_term in enumerate(subtask_searches, 1):
        print(f"  ðŸ¤– Subagent {i}: Researching {search_term}")
        results = search_web(search_term, 2)

        sources = []
        for result in results:
            if result.text and len(result.text) > 200:
                sources.append({
                    "title": result.title,
                    "content": result.text[:300]
                })

        subagent_results.append({
            "subtask": i,
            "search_focus": search_term,
            "sources": sources
        })

    total_sources = sum(len(r["sources"]) for r in subagent_results)
    print(f"  ðŸ“Š Combined: {total_sources} sources from {len(subagent_results)} agents")

    # Step 3: Lead Agent - Synthesis
    print("\nðŸ‘¨â€ðŸ’¼ LEAD AGENT: Synthesizing parallel findings...")

    # Combine all subagent findings
    synthesis_context = f"ORIGINAL QUERY: {query}\n\nSUBAGENT FINDINGS:\n"
    for result in subagent_results:
        synthesis_context += f"\nSubagent {result['subtask']} ({result['search_focus']}):\n"
        for source in result['sources'][:2]:  # Limit for brevity
            synthesis_context += f"- {source['title']}: {source['content']}...\n"

    synthesis_prompt = f"""{synthesis_context}

As the Lead Agent, synthesize these parallel findings into a comprehensive report:

EXECUTIVE SUMMARY:
[2-3 sentences covering the most important insights across all subagents]

INTEGRATED FINDINGS:
â€¢ [Key finding from foundational research]
â€¢ [Key finding from recent developments]
â€¢ [Key finding from applications research]
â€¢ [Cross-cutting insight that emerged]

RESEARCH QUALITY:
- Sources analyzed: {total_sources} across {len(subagent_results)} specialized agents
- Coverage: [How well the subtasks covered the topic]"""

    final_synthesis = ask_ai(synthesis_prompt)

    print("\n" + "=" * 50)
    print("ðŸŽ¯ MULTI-AGENT RESEARCH COMPLETE")
    print("=" * 50)
    print(final_synthesis)

    return {
        "query": query,
        "subagents": len(subagent_results),
        "total_sources": total_sources,
        "synthesis": final_synthesis
    }

# Streaming Chat Function
async def stream_chat(request: ChatRequest) -> AsyncGenerator[str, None]:
    """Stream chat responses from Cerebras"""
    try:
        # Build messages array with conversation history
        messages = []

        # Add system message
        messages.append({
            "role": "system",
            "content": """You are a helpful mathematics tutor and research assistant. Provide clear, accurate explanations and help students understand mathematical concepts.

IMPORTANT: Always use LaTeX for mathematical expressions:
- Inline math: $x^2 + 5x + 6$
- Display math: $$\\frac{-b \\pm \\sqrt{b^2-4ac}}{2a}$$
- Use proper LaTeX syntax for all equations, formulas, and mathematical symbols."""
        })

        # Add conversation history
        for msg in request.conversation_history:
            messages.append({
                "role": msg.role,
                "content": msg.content
            })

        # Add current user message
        messages.append({
            "role": "user",
            "content": request.message
        })

        # Create streaming completion
        stream = client.chat.completions.create(
            messages=messages,
            model="llama-4-scout-17b-16e-instruct",
            stream=True,
            max_tokens=1500,
            temperature=0.7
        )

        # Stream the response chunks
        for chunk in stream:
            if chunk.choices[0].delta.content:
                content = chunk.choices[0].delta.content
                # Format as SSE (Server-Sent Events)
                yield f"data: {json.dumps({'content': content})}\n\n"

        # Send done signal
        yield f"data: {json.dumps({'done': True})}\n\n"

    except Exception as e:
        error_msg = f"Streaming error: {str(e)}"
        yield f"data: {json.dumps({'error': error_msg})}\n\n"

@app.post("/chat/stream", tags=["chat"])
async def chat_stream(request: ChatRequest):
    """
    Streaming chat endpoint with conversation history

    - Accepts user message and conversation history
    - Streams AI responses in real-time using SSE
    - Maintains context across conversation turns
    - Uses Cerebras LLM for fast inference
    """
    return StreamingResponse(
        stream_chat(request),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
        }
    )

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)